#!/usr/bin/env python3
"""
æµ‹è¯•JORA modules_to_saveå…¼å®¹æ€§ä¿®å¤
"""

import sys
import os
import torch
import torch.nn as nn

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from peft.tuners.jora.config import JoraConfig
from peft.tuners.jora.model import JoraModel


def test_modules_to_save_compatibility():
    """æµ‹è¯•modules_to_saveçš„å…¼å®¹æ€§ä¿®å¤"""
    print("ğŸ§ª æµ‹è¯•JORA modules_to_saveå…¼å®¹æ€§ä¿®å¤...")

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(10, 20)
            self.linear2 = nn.Linear(20, 30)
            self.lm_head = nn.Linear(30, 1000)  # æ¨¡æ‹Ÿlm_head

        def forward(self, x):
            x = self.linear1(x)
            x = self.linear2(x)
            x = self.lm_head(x)
            return x

    base_model = TestModel()

    # åˆ›å»ºJORAé…ç½®ï¼ŒåŒ…å«modules_to_save
    config = JoraConfig(
        target_modules=["linear1", "linear2"],
        modules_to_save=["lm_head"]  # lm_headåº”è¯¥ä¿æŒå¯è®­ç»ƒ
    )

    # åˆ›å»ºJORAæ¨¡å‹
    jora_model = JoraModel(base_model, config, "test")

    print("  é…ç½®æ£€æŸ¥:")
    print(f"    target_modules: {config.target_modules}")
    print(f"    modules_to_save: {config.modules_to_save}")

    # è®°å½•åˆå§‹å‚æ•°çŠ¶æ€
    initial_requires_grad = {}
    for name, param in jora_model.named_parameters():
        initial_requires_grad[name] = param.requires_grad

    print("\n  åˆå§‹å‚æ•°çŠ¶æ€æ£€æŸ¥:")
    lm_head_params = [name for name in initial_requires_grad.keys() if 'lm_head' in name]
    jora_params = [name for name in initial_requires_grad.keys() if any(prefix in name for prefix in ['theta_L', 'theta_R', 'core', 'ecd_log_mag'])]
    other_params = [name for name in initial_requires_grad.keys() if name not in lm_head_params + jora_params]

    print(f"    lm_headå‚æ•°æ•°é‡: {len(lm_head_params)}")
    print(f"    JORAå‚æ•°æ•°é‡: {len(jora_params)}")
    print(f"    åŸºç¡€æ¨¡å‹å‚æ•°æ•°é‡: {len(other_params)}")

    # è°ƒç”¨_mark_only_adapters_as_trainable
    print("\n  è°ƒç”¨_mark_only_adapters_as_trainable...")
    jora_model._mark_only_adapters_as_trainable(jora_model.model)

    # æ£€æŸ¥å‚æ•°çŠ¶æ€
    final_requires_grad = {}
    for name, param in jora_model.named_parameters():
        final_requires_grad[name] = param.requires_grad

    print("\n  æœ€ç»ˆå‚æ•°çŠ¶æ€æ£€æŸ¥:")

    # æ£€æŸ¥lm_headå‚æ•°ï¼ˆåº”è¯¥ä¿æŒå¯è®­ç»ƒï¼‰
    lm_head_trainable = [name for name in lm_head_params if final_requires_grad[name]]
    lm_head_frozen = [name for name in lm_head_params if not final_requires_grad[name]]

    print(f"    lm_headå¯è®­ç»ƒå‚æ•°: {len(lm_head_trainable)}")
    print(f"    lm_headå†»ç»“å‚æ•°: {len(lm_head_frozen)}")

    if lm_head_frozen:
        print(f"    âŒ å†»ç»“çš„lm_headå‚æ•°: {lm_head_frozen[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
        return False

    # æ£€æŸ¥JORAå‚æ•°ï¼ˆåº”è¯¥å¯è®­ç»ƒï¼‰
    jora_trainable = [name for name in jora_params if final_requires_grad[name]]
    jora_frozen = [name for name in jora_params if not final_requires_grad[name]]

    print(f"    JORAå¯è®­ç»ƒå‚æ•°: {len(jora_trainable)}")
    print(f"    JORAå†»ç»“å‚æ•°: {len(jora_frozen)}")

    if jora_frozen:
        print(f"    âŒ å†»ç»“çš„JORAå‚æ•°: {jora_frozen[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
        return False

    # æ£€æŸ¥åŸºç¡€æ¨¡å‹å‚æ•°ï¼ˆåº”è¯¥å†»ç»“ï¼Œé™¤äº†modules_to_saveä¸­çš„ï¼‰
    base_model_trainable = [name for name in other_params if final_requires_grad[name]]

    print(f"    åŸºç¡€æ¨¡å‹å¯è®­ç»ƒå‚æ•°: {len(base_model_trainable)}")

    if base_model_trainable:
        print(f"    âŒ åŸºç¡€æ¨¡å‹ä¸­ä¸åº”è¯¥å¯è®­ç»ƒçš„å‚æ•°: {base_model_trainable[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
        return False

    print("âœ… modules_to_saveå…¼å®¹æ€§ä¿®å¤æµ‹è¯•é€šè¿‡!")
    print("    - lm_headå‚æ•°æ­£ç¡®ä¿æŒå¯è®­ç»ƒ")
    print("    - JORAå‚æ•°æ­£ç¡®ä¿æŒå¯è®­ç»ƒ")
    print("    - å…¶ä»–å‚æ•°æ­£ç¡®å†»ç»“")

    return True


def test_without_modules_to_save():
    """æµ‹è¯•ä¸ä½¿ç”¨modules_to_saveçš„æƒ…å†µ"""
    print("\nğŸ§ª æµ‹è¯•ä¸ä½¿ç”¨modules_to_saveçš„æƒ…å†µ...")

    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear1 = nn.Linear(10, 20)
            self.linear2 = nn.Linear(20, 30)

        def forward(self, x):
            x = self.linear1(x)
            x = self.linear2(x)
            return x

    base_model = TestModel()

    # åˆ›å»ºJORAé…ç½®ï¼Œä¸åŒ…å«modules_to_save
    config = JoraConfig(
        target_modules=["linear1", "linear2"]
    )

    # åˆ›å»ºJORAæ¨¡å‹
    jora_model = JoraModel(base_model, config, "test")

    # è°ƒç”¨_mark_only_adapters_as_trainable
    jora_model._mark_only_adapters_as_trainable(jora_model.model)

    # æ£€æŸ¥æ‰€æœ‰å‚æ•°çŠ¶æ€
    trainable_params = []
    frozen_params = []
    for name, param in jora_model.named_parameters():
        if param.requires_grad:
            trainable_params.append(name)
        else:
            frozen_params.append(name)

    # JORAå‚æ•°åº”è¯¥å¯è®­ç»ƒï¼Œå…¶ä»–å‚æ•°åº”è¯¥å†»ç»“
    jora_trainable = [name for name in trainable_params if any(prefix in name for prefix in ['theta_L', 'theta_R', 'core', 'ecd_log_mag'])]
    non_jora_trainable = [name for name in trainable_params if name not in jora_trainable]

    print(f"  å¯è®­ç»ƒå‚æ•°æ€»æ•°: {len(trainable_params)}")
    print(f"  JORAå¯è®­ç»ƒå‚æ•°: {len(jora_trainable)}")
    print(f"  éJORAå¯è®­ç»ƒå‚æ•°: {len(non_jora_trainable)}")

    # ä¸ä½¿ç”¨modules_to_saveæ—¶ï¼Œæ‰€æœ‰éJORAå‚æ•°éƒ½åº”è¯¥è¢«å†»ç»“
    if len(non_jora_trainable) == 0:
        print("âœ… ä¸ä½¿ç”¨modules_to_saveæ—¶æ­£ç¡®å†»ç»“æ‰€æœ‰éJORAå‚æ•°")
        return True
    else:
        print("âŒ å­˜åœ¨ä¸åº”è¯¥å¯è®­ç»ƒçš„å‚æ•°")
        print(f"  ä¸åº”è¯¥å¯è®­ç»ƒçš„å‚æ•°: {non_jora_trainable[:3]}...")
        return False


if __name__ == '__main__':
    success1 = test_modules_to_save_compatibility()
    success2 = test_without_modules_to_save()

    overall_success = success1 and success2
    print(f"\nğŸ† æ€»ä½“ç»“æœ: {'é€šè¿‡' if overall_success else 'å¤±è´¥'}")
    sys.exit(0 if overall_success else 1)