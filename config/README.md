# PEFTé…ç½®æ–‡ä»¶è¯´æ˜

## ğŸ“ é…ç½®æ–‡ä»¶ç›®å½•

æ­¤ç›®å½•åŒ…å«å„ç§PEFTæ–¹æ³•çš„é…ç½®æ–‡ä»¶ï¼Œç”¨äºä¸åŒçš„æ¨¡å‹å’Œä»»åŠ¡ã€‚

## ğŸ”§ LoRA Llama2-7B Rank4 é…ç½®

### æ–‡ä»¶ï¼š`lora_llama2_7b_rank4.json`

**é€‚ç”¨åœºæ™¯ï¼š** å¾®è°ƒLlama2-7Bæ¨¡å‹ï¼Œä½¿ç”¨LoRAæ–¹æ³•ï¼Œrank=4

### å‚æ•°è¯´æ˜ï¼š

| å‚æ•° | å€¼ | è¯´æ˜ |
|-----|-----|------|
| `peft_type` | `"LORA"` | PEFTæ–¹æ³•ç±»å‹ |
| `task_type` | `"CAUSAL_LM"` | ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹ |
| `r` | `4` | LoRAç§©ï¼ˆrankï¼‰- è¾ƒä½çš„ç§©å‡å°‘å‚æ•° |
| `target_modules` | `["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]` | Llamaæ¨¡å‹çš„æ³¨æ„åŠ›å±‚å’ŒFFNå±‚ |
| `lora_alpha` | `8` | LoRAç¼©æ”¾å› å­ï¼ˆé€šå¸¸ä¸ºrankçš„2å€ï¼‰ |
| `lora_dropout` | `0.1` | LoRA dropoutæ¦‚ç‡ |
| `bias` | `"none"` | ä¸è®­ç»ƒbiaså‚æ•° |
| `inference_mode` | `true` | æ¨ç†æ¨¡å¼ä¼˜åŒ– |

### ä½¿ç”¨ç¤ºä¾‹ï¼š

```python
from peft import PeftModel, LoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
import json

# åŠ è½½é…ç½®æ–‡ä»¶
with open('config/lora_llama2_7b_rank4.json', 'r') as f:
    config_dict = json.load(f)

# åˆ›å»ºLoRAé…ç½®
lora_config = LoraConfig(**config_dict)

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "/mnt/sda/jqh/pretrained_checkpoints/Llama-2-7b-hf/",
    device_map="auto"
)

# åº”ç”¨LoRA
model = PeftModel(model, lora_config)

# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
```

### å‚æ•°è°ƒä¼˜å»ºè®®ï¼š

#### **Ranké€‰æ‹©ï¼š**
- **Rank 4**: æœ€å°çš„å‚æ•°é‡ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯
- **Rank 8**: å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡çš„æ¨èé€‰æ‹©
- **Rank 16**: æ›´å¥½çš„æ€§èƒ½ï¼Œå‚æ•°é‡é€‚ä¸­

#### **Alphaè®¾ç½®ï¼š**
- é€šå¸¸è®¾ç½®ä¸ºrankçš„2å€ï¼š`alpha = 2 * rank`
- å¯ä»¥æ ¹æ®ä»»åŠ¡è°ƒæ•´ï¼šæ›´éš¾çš„ä»»åŠ¡å¯ä»¥ä½¿ç”¨æ›´é«˜çš„alpha

#### **Target Modulesï¼š**
- **æ³¨æ„åŠ›å±‚**: `q_proj`, `k_proj`, `v_proj`, `o_proj`
- **å‰é¦ˆç½‘ç»œ**: `gate_proj`, `up_proj`, `down_proj`
- **åµŒå…¥å±‚**: å¯é€‰æ‹©æ€§æ·»åŠ  `embed_tokens`, `lm_head`

### å…¶ä»–é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼š

åç»­å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šé…ç½®æ–‡ä»¶ï¼š
- ä¸åŒrankçš„LoRAé…ç½®
- å…¶ä»–PEFTæ–¹æ³•ï¼ˆBoFT, OFT, IA3ç­‰ï¼‰
- ä¸åŒä»»åŠ¡ç±»å‹çš„é…ç½®

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1ï¼šä½¿ç”¨è®­ç»ƒåŒ…è£…è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨PEFTé…ç½®æ–‡ä»¶è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒå‘½ä»¤
python train_with_config.py \
    --model_path "/mnt/sda/jqh/pretrained_checkpoints/Llama-2-7b-hf/" \
    --dataset_name "yahma/alpaca-cleaned" \
    --config "config/lora_llama2_7b_rank4.json" \
    --output_dir "checkpoints/llama2_7b_lora_rank4_alpaca" \
    --num_epochs 3

# ç›´æ¥æ‰§è¡Œè®­ç»ƒï¼ˆæ·»åŠ --executeå‚æ•°ï¼‰
python train_with_config.py \
    --model_path "/mnt/sda/jqh/pretrained_checkpoints/Llama-2-7b-hf/" \
    --dataset_name "yahma/alpaca-cleaned" \
    --config "config/lora_llama2_7b_rank4.json" \
    --output_dir "checkpoints/llama2_7b_lora_rank4_alpaca" \
    --num_epochs 3 \
    --execute
```

### æ–¹æ³•2ï¼šæ‰‹åŠ¨æ‰§è¡Œç”Ÿæˆçš„å‘½ä»¤

```bash
# å¤åˆ¶ä¸Šè¿°è„šæœ¬ç”Ÿæˆçš„å‘½ä»¤æ‰‹åŠ¨æ‰§è¡Œ
python examples/sft/train.py \
    --seed 42 \
    --model_name_or_path "/mnt/sda/jqh/pretrained_checkpoints/Llama-2-7b-hf/" \
    --dataset_name "yahma/alpaca-cleaned" \
    --chat_template_format "none" \
    --add_special_tokens False \
    --append_concat_token False \
    --splits "train" \
    --max_length 2048 \
    --num_train_epochs 3 \
    --logging_steps 10 \
    --log_level "info" \
    --logging_strategy "steps" \
    --eval_strategy "no" \
    --save_strategy "epoch" \
    --bf16 True \
    --packing False \
    --learning_rate 0.0002 \
    --lr_scheduler_type "cosine" \
    --weight_decay 0.01 \
    --warmup_ratio 0.03 \
    --max_grad_norm 1.0 \
    --output_dir "checkpoints/llama2_7b_lora_rank4_alpaca" \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --gradient_checkpointing True \
    --use_reentrant True \
    --dataset_text_field "text" \
    --use_peft_lora True \
    --lora_r 4 \
    --lora_alpha 8 \
    --lora_dropout 0.1 \
    --lora_target_modules "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj" \
    --use_flash_attn True
```

## ğŸ“‹ æ³¨æ„äº‹é¡¹ï¼š

1. **æ¨¡å‹å…¼å®¹æ€§**: æ­¤é…ç½®ä¸“é—¨é’ˆå¯¹Llama2-7Bæ¨¡å‹ä¼˜åŒ–
2. **å†…å­˜ä½¿ç”¨**: rank=4æ˜¯æœ€èŠ‚çœå†…å­˜çš„è®¾ç½®
3. **æ€§èƒ½å¹³è¡¡**: åœ¨å‚æ•°æ•ˆç‡å’Œä»»åŠ¡æ€§èƒ½é—´å–å¾—å¹³è¡¡
4. **æ‰©å±•æ€§**: å¯ä»¥ä½œä¸ºæ¨¡æ¿ä¿®æ”¹ç”¨äºå…¶ä»–Llamaæ¨¡å‹
5. **é…ç½®æ–‡ä»¶**: è®­ç»ƒæ—¶ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œé…ç½®æ–‡ä»¶ä¸»è¦ç”¨äºä¿å­˜å’ŒåŠ è½½æ¨¡å‹
