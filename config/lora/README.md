# LoRA å®éªŒé…ç½®æ–‡ä»¶

æ ¹æ® TODO.md å®éªŒè®¾è®¡ç”Ÿæˆçš„ LoRA é…ç½®æ–‡ä»¶å’Œè®­ç»ƒå‘½ä»¤ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

### é…ç½®æ–‡ä»¶ (16ä¸ª)
å‘½åè§„åˆ™: `lora_{model}_{dataset}_rank{rank}.json`

**æ¨¡å‹**: `llama2_7b`, `mistral_7b`
**æ•°æ®é›†**: `alpaca`, `gsm8k`
**Rank**: 4, 8, 16, 32

### é…ç½®è¯¦æƒ…

- **ç›®æ ‡æ¨¡å—**: `["q_proj","v_proj"]` (è½¨é“A)
- **Alpha**: 2 Ã— rank
- **Dropout**: 0.05
- **å…¶ä»–å‚æ•°**: å‚è€ƒ TODO.md ç»Ÿä¸€è®¾ç½®

## ğŸš€ è®­ç»ƒå‘½ä»¤

### è„šæœ¬ä½ç½®
`scripts/run_lora_experiments.sh`

### å®éªŒè®¾ç½®
- **æ¯ä¸ªé…ç½®**: 3ä¸ªéšæœºç§å­ (42, 1337, 2026)
- **æ€»è®­ç»ƒå‘½ä»¤**: 16Ã—3 = 48 ä¸ª
- **å­¦ä¹ ç‡**:
  - alpaca-cleaned: 2e-4 (SFT-S)
  - gsm8k: 1e-4 (SFT-M)

### è¿è¡Œæ–¹å¼
```bash
# è¿è¡Œæ‰€æœ‰å®éªŒ
bash scripts/run_lora_experiments.sh

# æˆ–è¿è¡Œå•ä¸ªå‘½ä»¤ (ä»è„šæœ¬ä¸­å¤åˆ¶)
CUDA_VISIBLE_DEVICES=1 python train_with_config.py \
    --model_path "/mnt/sda/jqh/pretrained_checkpoints/Llama-2-7b-hf/" \
    --dataset_name "yahma/alpaca-cleaned" \
    --config "config/lora/lora_llama2_7b_alpaca_rank4.json" \
    --output_dir "checkpoints/lora_llama2_7b_alpaca_rank4_seed42" \
    --num_epochs 3 \
    --batch_size 2 \
    --learning_rate 0.0002 \
    --execute --disable_wandb
```

## ğŸ“Š å‚æ•°é¢„ç®—

| é…ç½® | å¯è®­ç»ƒå‚æ•° | ä¸ LoRA rank å¯¹åº”å…³ç³» |
|------|-----------|----------------------|
| rank 4 | ~800K | ç›¸å½“äº LoRA râ‰ˆ4 |
| rank 8 | ~1.6M | ç›¸å½“äº LoRA râ‰ˆ8 |
| rank 16 | ~3.2M | ç›¸å½“äº LoRA râ‰ˆ16 |
| rank 32 | ~6.4M | ç›¸å½“äº LoRA râ‰ˆ32 |

## ğŸ” æ³¨æ„äº‹é¡¹

1. **è½¨é“é€‰æ‹©**: ä½¿ç”¨è½¨é“A `["q_proj","v_proj"]` ä»¥ç¡®ä¿å…¬å¹³å¯¹æ¯”
2. **å‚æ•°åŒ¹é…**: rank é€‰æ‹©ä¸ TODO.md ä¸­çš„ JORA å‚æ•°é¢„ç®—å¯¹åº”
3. **ç§å­è®¾ç½®**: æ¯ä¸ªé…ç½®ä½¿ç”¨ä¸åŒéšæœºç§å­ç¡®ä¿ç»“æœå¯é æ€§
4. **è¾“å‡ºç›®å½•**: è‡ªåŠ¨åŒ…å«é…ç½®ä¿¡æ¯ï¼Œä¾¿äºåç»­åˆ†æ

## ğŸ“ˆ é¢„æœŸç»“æœ

è¿™äº›å®éªŒå°†ä¸º JORA æ–¹æ³•æä¾› baseline å¯¹æ¯”ï¼Œå¸®åŠ©éªŒè¯ï¼š
- JORA çš„å‡ ä½•ä¿æŒä¼˜åŠ¿
- ä¸åŒæ¨¡å‹æ¶æ„å¯¹ PEFT æ–¹æ³•æ•ˆæœçš„å½±å“
- å‚æ•°é¢„ç®—å¯¹æ€§èƒ½çš„å½±å“
